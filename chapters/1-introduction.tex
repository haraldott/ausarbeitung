\chapter{Introduction\label{cha:introduction}}


\section{Motivation\label{sec:motivation}}
The Internet is permeating almost every aspect of modern human life. Large numbers of online services ease our ways of retrieving information, purchasing goods and staying in contact with each other. New opportunities for businesses emerge with the availability of reliable and easy to use public cloud infrastructures. These cloud systems are environments which make it possible to abstract and share distributed hardware resources, allowing the operation of vast numbers of multiple simulated environments on hardware systems. Virtualisation allows the separate, yet simultaneous allocation of resources for various services at once.

As these cloud systems are becoming increasingly complex, they are getting harder to maintain and to operate, with system failures, outages and other unwanted behaviour occurring on a regular basis. Detecting such failures is indispensable for the correct, safe and reliable operation of complex systems, with the complete outage of the paying system of an E-Commerce shop for example, potentially resulting in high losses in revenue and the disruption of user experience. The software which operates these cloud environments, like most software, produces log data during execution - text-strings, which contain information about actions that have been performed, but can also indicate the state of a system at a given point in time. Log files can be used to conduct failure and anomaly analysis, and to help understand the root causes of failures and errors. System operators would examine logs manually and determine if certain log events can be linked to a given system failure. At the scale of mentioned systems, analysing such log files manually is infeasible. It is therefore necessary to develop automised methods for this purpose. Naive approaches like matching certain keywords (e.g. "\textit{error}"), constructing a set of log lines indicating anomalies or regular expressions are not adequate to capture the complex nature of anomalies. For example, errors can happen and can even be output explicitly as errors, but at the same time it can be normal behaviour of a system to then automatically recover from given errors, so triggering an alarm is not wanted in these cases \cite{meng2019loganomaly}. Traditional anomaly detection based on standard mining technologies cannot cope with the complex nature of anomalies in modern systems \cite{du2017deeplog}, since they are constantly evolving, which would require constant adjustments. Therefore, a more general approach is desirable.





\section{Scope\label{sec:scope}}
The scope of this work is to find an appropriate way to represent dynamically changing log events using language models, and to assess their quality with regards to their ability of being utilised for solving the problem of anomaly detection. Therefore, an approach using an auto encoder to learn fixed length representations of log events that have been transformed into word embeddings with GloVe, is presented as baseline. Next, an approach to utilising word embeddings obtained from Bert are compared to those obtained from GPT-2 to use as input for a LSTM to  learn error-free log sequences in a both supervised and unsupervised manner. For this purpose, three different ways of mapping the input of multiple log events to a target space, namely binary classification, multi-class classification and regression are evaluated.

%TODO big picture

\section{Outline\label{sec:outline}}


This master's thesis is separated into 7 chapters.
\\
\\
\textbf{Chapter \ref{cha:background}} presents background knowledge and terms required in order to understand the presented concept.
\\
\\
\textbf{Chapter \ref{cha:concept}} presents the formal problem definition, the requirements and assumptions, followed by the developed concept. The complete pipeline is explained in detail, clarifying the necessity of every step in order to assemble the full model. Moreover, constraints of the model and suggestions for possible improvements and further refinement of the model are given.
\\
\\
In \textbf{chapter \ref{cha:results}}, the experimental set-up, followed by the results of the evaluation are presented. In doing so, a detailed evaluation of the used language model and a comparison between them is conducted.
\\
\\
\textbf{Chapter \ref{cha:related_work}} describes related work, pointing out similarities and differences to the presented concept.
\\
\\
In \textbf{chapter \ref{cha:conclusion}}, the final conclusion is presented.



