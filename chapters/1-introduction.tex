\chapter{Introduction\label{cha:introduction}}


\section{Motivation\label{sec:motivation}}
The Internet is permeating almost every aspect of modern human life. Large numbers of online services ease our ways of retrieving information, purchasing goods and staying in contact with each other. New opportunities for businesses emerge with the availability of reliable and easy to use public cloud infrastructures. These cloud systems are environments which make it possible to abstract and share distributed hardware resources, allowing the operation of vast numbers of multiple simulated environments on hardware systems. Virtualisation allows the separate, yet simultaneous allocation of resources for various services at once.

As these cloud systems are becoming increasingly complex, they are getting harder to maintain and to operate, with system failures, outages and other unwanted behaviour occurring on a regular basis. Detecting such failures is indispensable for the correct, safe and reliable operation of complex systems. The complete outage of the paying system of an E-Commerce shop for example, can potentially result in high losses in revenue and the disruption of user experience. The software which operates these cloud environments, like most software, produces log data during execution - text-strings, which contain information about interactions between data, files, services and applications. Log files can be used to conduct failure and anomaly analysis, and to help understand the root causes of failures and errors. System operators would examine logs manually and determine if certain log events can be linked to a given system failure. At the scale of mentioned systems, analysing such log files manually is infeasible. It is therefore necessary to develop automised methods for this purpose. Naive approaches like matching certain keywords (e.g. "\textit{error}"), constructing a set of log lines indicating anomalies or regular expressions are not adequate to capture the complex nature of anomalies. For example, errors can happen and can even be output explicitly as errors, but at the same time it can be normal behaviour of a system to then automatically recover from given errors, so triggering an alarm is not wanted in these cases \cite{meng2019loganomaly}. 
There exist several cases where even more complex environments can be on hand for anomaly detection. Typically, a reduction in the performance of an anomaly detection model can arise in two cases: When the system or its services are re-configured or updated in terms of operating system, library version or network, or when the model is trained on an existing system and later deployed on a new, similar system. New data and business characteristics arise and have to be met. In any of these cases, the model has to be dynamically adapted to the new environment and settings. In both of the above mentioned cases, the models presented by previous works are not able to adapt properly, thus resulting in poor generalisation of the model.

Logs are constantly evolving due to the fact that developers are modifying source code frequently, including logging statements, thus leading to changing log data. Kabinna et al. \cite{kabinna2018examining} found that 20\%-45\% of logging statements in the studied open source applications change throughout their lifetime. Due to the constant evolution of logging data, the performance of existing anomaly detection approaches is significantly reduced. Existing approaches assume a constancy in logs which is not realistic in real-world systems, where unstable log events containing changes on events and sequences appear constantly. Even small semantically insignificant changes to known log events would be identified as a completely different log event \cite{du2017deeplog} \cite{meng2019loganomaly} \cite{zhang2016automated}. 

Therefore, existing approaches will either fail to work due to their incapacity to cope with unseen log events, or produce incorrect classification results. Additionally, normal log sequences are also likely to change due to new execution paths, processing noise or delays. It is infeasible to continuously update and retrain log-based anomaly detection tools due to the large amount of effort it requires \cite{zhang2019robust}. Therefore, a more general approach is desirable, which is robust to the changes mentioned above and can be transferable to new systems.



%Traditional anomaly detection based on standard mining technologies cannot cope with the complex nature of anomalies in modern systems \cite{du2017deeplog}, since they are constantly evolving, which would require constant adjustments. 





\section{Scope\label{sec:scope}}
Large amounts of log data have renewed the interest in developing one-class deep learning methods to extract general patterns from non-anomalous log data. This work proposes to establish a connection between the latest advances in natural language processing and anomaly detection in log data. The need for using language models to transform log events into numerical vector embeddings arises from the weaknesses of older anomaly detection approaches that are not able to generalise well on new logs and cope with the challenges mentioned above. Difficulties exist in reusing previously obtained knowledge from a given dataset and transferring it to a differently structured dataset. This work provides a means to automatically detect anomalies in logs that are potentially affected by processing noise and changes of log events by updates of the underlying software and to transfer knowledge obtained from a dataset of log sequences to another dataset of logs with a transfer method.

In particular this includes the following points
\begin{enumerate}
	\item A model where numerical log vector representations are obtained from general purpose language models and are utilised in a bi-directional long short-term memory (Bi-LSTM) neural network for anomaly detection
	\item Two prediction modes, including a regression-based approach and a classification-based approach are proposed and evaluated.
	\item Experiments including scenarios to evaluate the robustness of the obtained log vector representations by altering log messages and sequences of log messages.
	\item Transfer of the method to another dataset by simulating a upgraded version of the system and fine-tuning of the model.
\end{enumerate}

%The scope of this work is to find an appropriate way to represent dynamically changing log events using language models, and to assess their quality with regards to their ability of being utilised for solving the problem of anomaly detection. Therefore, an approach using an auto encoder to learn fixed length representations of log events that have been transformed into word embeddings with GloVe, is presented as baseline. Next, an approach to utilising word embeddings obtained from Bert, GPT-2 and XL-Transformers as input for a LSTM to  learn error-free log sequences in a both supervised and unsupervised manner. For this purpose, two different ways of mapping the input of multiple log events to a target space are evaluated, namely classification and regression.

%TODO big picture

\section{Outline\label{sec:outline}}


This master's thesis is separated into 7 chapters.
\\
\\
\textbf{Chapter \ref{cha:background}} presents background knowledge and terms required in order to understand the presented concept.
\\
\\
\textbf{Chapter \ref{cha:concept}} presents the formal problem definition, the requirements and assumptions, followed by the developed concept. The complete pipeline is explained in detail, clarifying the necessity of every step in order to assemble the full model.
\\
\\
In \textbf{chapter \ref{cha:results}}, the experimental set-up, followed by the results of the evaluation are presented. In doing so, a detailed evaluation of the used language model and a comparison between them is conducted.
\\
\\
\textbf{Chapter \ref{cha:related_work}} describes related work, presenting an overview of recent studies, the current state of research and presents three important contributions in detail.
\\
\\
In \textbf{chapter \ref{cha:conclusion}}, the final conclusion of the work is presented.



