\chapter{Conclusion\label{cha:conclusion}}



\section{Summary\label{sec:summary}}
This work addresses the problem of log anomaly detection in large-scale distributed systems such as the cloud, as an essential part of a utility for improving the reliability and the security of a complex system. The generalisation problem for anomaly detection on unseen logs has been addressed by introducing a language model agnostic framework which is able to use pre-trained language models for obtaining log vector representations as a log representation technique. An Bi-LSTM neural network is then used as a method for anomaly detection. With this,  the log message semantics and the sequential nature of the log messages are captured. It is shown, that the proposed model is more robust to alteration in the log messages -- scenarios frequently occurring in practice due to software updates, deployment of new services or systems. The results show that the  framework can achieve high F1-scores and precision, using the state-of-the-art language models. Furthermore, we show that not every representation is equally useful for anomaly detection as some of the language models fail to generate log representations that can be separated by a learned decision boundary. 

The proposed approach opens new potential for anomaly detection not just from log data, but from other sources that have the notion of a distributed representation of an event e.g., distributed tracing data. The proposed method will hopefully motivate further research in the direction of development of pre-trained language models on logs. This would enhance the log repersentations, and thus, improve the performance of the anomaly detection methods.


%This work addresses to the problem of anomaly detection using system logs in large-scale distributed systems, thus making an essential contribution to improving the reliability and the security of a distributed system. Finding suitable representations for log data in the form of word embeddings is an important step towards building a robust, environment-agnostic anomaly detection model. In this work, three different language models, namely Bert, GPT-2 and XL-Transformers are compared and used with regression-based and a classification-based approach for anomaly detection. Additionally, the language models are evaluated with regards to their robustness to small permutations in the normal trace of logs, a scenario often occurring in practice due to caching, hardware problems and transmission delays. Finally, an evaluation is conducted with regards to the possibility of transferring obtained knowledge from a dataset to a different one, by combining different degrees of alterations in order to simulate another dataset.

%The work done can be summarised by the following steps:
%\begin{itemize}
%		\item Finding a suitable log parser by evaluating the available log parsers on their performance on the log dataset.
%		\item Finding suitable language models in order to represent log templates so that they can be used for the task of anomaly detection
%		\item Finding a suitable neural network design for anomaly detection.
%		\item Implementing the regression-based and classification-based methods for anomaly detection.
%		\item Finding a means to alter log datasets in such a way that the existence of a different log dataset and the evolution of log data can be simulated in oder to evaluate the performance of the transfer of knowledge method.
%		\item Evaluation of the proposed model by comparing the quality of word embeddings obtained by different pre-trained language models with regards to the task of anomaly detection in system logs.
%\end{itemize}


%\section{Problems Encountered\label{sec:problems}}
%Several impediments occurred during the development of the model. 

%At the beginning, it was not obvious, which word embeddings to use in order to represent log events. First attempts were made with GloVe, using word vectors that were trained only the available small log corpus, since the publicly available pre-trained vectors did not have representations for log-domain-specific words like "MB", "GB", "deallocate", "VM". These representations of log events of unequal length were padded with zeros and fed into an Auto Encoder, in order to learn the sentence representations. Then, the latent space representation of every log event was used in order to obtain representations of equal length. This attempt yielded acceptable results on the easiest case of injecting anomalies without alterations, but were disappointing when injecting alterations, making it unfit for the transfer of knowledge. This was unfortunate, since a lot of time was invested into this approach. Using more sophisticated language models like Bert delivered better results overall.

%Finding appropriate hyperparameters for the model, for example sequence length, number of hidden units and number of layers for the LSTM or clipping, was not easy in the beginning, since all of them heavily influence the end result if not set correctly. A lot of development time was invested, trying to find the reasons for problems elsewhere, when the only problem was for example a wrongly chosen value for gradient clipping.

\section{Outlook\label{sec:outlook}}
In order to have more means to evaluate different language models, next to the regression and classification approach, a binary classification approach could be implemented.

The word embeddings are taken from the used language models as they are. Finetuning on the log corpora is only conducted using the default tasks that have been used to train the language model on large corpora. The corpora on log data are likely to be too small. Even though most log events are sentences in English, they use very reduced idioms. A deeper investigation on how to train the language model specifically on the task of anomaly detection could potentially be useful in order to obtain better results. Integrating the fine-tuning step into the model's pipeline, can potentially improve the quality of the word embeddings. This can be especially useful for the transfer of knowledge task.

A very important step in order to make the proposed model even more robust, resilient and most importantly able to transfer knowledge on new datasets, evaluation into implementing an attention mechanism, as described by Vaswani et al. \cite{vaswani2017attention}, that fits the use-case correctly and enhances it. The mentioned papers in \ref{cha:related_work} have proven that using an attention mechanism in combination with a LSTM can highly improve the results obtained from the model.


