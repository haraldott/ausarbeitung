\chapter{Concept \label{cha:concept}}
Establishing a connection between the latest advances in NLP and anomaly detection in system log data is a recently emerged field in research. The need for using language models to convert log events into word embeddings emerges from the weaknesses of older anomaly detection approaches that are not flexible enough to cope with the complex nature of system log data. There exist difficulties in reusing previously obtained knowledge from training on data from a given dataset and transferring it to a differently structured dataset. Most proposed techniques for solving the problem of anomaly detection in system log data suffer from not being transferable and being non-resilient to changing log data. The objective of this work is to provide a means to automatically detect anomalies in logs that are potentially affected by processing noise and changes of log events by updates of the underlying software, and to transfer knowledge obtained from a dataset of log sequences to another dataset of logs with a \textit{transfer of knowledge} method.

In section \ref{sec:problem_statement}, the general problem statement is defined, and necessary requirements for the model are specified. 
In the following section \ref{sec:overall_system}, the overall system architecture with its components is outlined and visualised.
In section \ref{sec:pre_processing} the necessary pre-processing steps for preparing the raw log messages for the anomaly prediction model are described in detail.
In section \ref{sec:prediction_model} the developed prediction approaches are described in detail. There exist two approaches, the regression and the classification approach, both using log event representations obtained by a language model.
In section \ref{sec:transferlearning} the transfer of knowledge mechanism is described.


\section{Problem Statement and Prerequisites \label{sec:problem_statement}}
Logs are print statements inside programs which are defined by a fixed sequence of code statements written by developers. The execution of a program is defined by these statements, and therefore follows a predetermined pattern. Hence, the produced logs must also follow certain patterns, chronological orders, and proportional relationships between number of occurrences of logs with each other.

\subsection{Formal problem definition \label{sec:formal_problem_definition}}
We define a system log $L$ as a sequence of log events or log messages $(m_0, m_1, ..., m_n)$ generated by logging instructions (e.g. \textit{printf()} or \textit{log.trace()}) within the software source code during the execution of the program. Log messages consist of a constant (log template) and a variable part.
Log messages consist of tokens - most tokens are English words, but do also include special characters. Each log message consists of a bounded sequence of tokens, $\mathbf{z_i}=(z_{j}\,:\,w \in  \mathbb{Z},\,j=1,2,...,|\mathbf{z_i}|)$, where $\mathbb{Z}$ is a set of all tokens, $j$ is the positional index of a token within the log message $m_i$, and $|\mathbf{z_i}|$ is the total number of tokens in $m_i$. For different $m_i$, $|\mathbf{z_i}|$ can vary. Depending on the concrete tokenisation method, $t$ can be a word, word piece, or character. Therefore, tokenisation is defined as a transformation function $\mathcal{T}: m_i \to \mathbf{t_i}, \forall i$.

We additionally introduce the notion a numerical vector representation (embedding vector). An embedding vector $e_i$ is a $d$-dimensional real valued vector representation of a log message template $t_i$. The embedding vector $e_i$ is obtained from a pre-trained language model $M(t_i)$.

\textit{Classification}: For every distinct log template $t_i$, we assign a class $c_i$. Let $g(E_i, \Psi): \left|s\right| \times \mathbb{R}^w \rightarrow \mathbb{R}^k,$ be a function computed by a neural network, that given a subsequence of $s$ vectors out of the dataset $E$, namely $E_i = (e_i, ..., e_{i+s})$, returns a probability distribution $Pr_i[e_{i+s+1}|E_i]$ = $(c_0: p_0, c_1: p_1, ..., c_k: p_k)$, describing the probability for each template class from $C$ to appear as the next class at index $i+s+1$, given $E_i$. The objective is to learn the parameters $\Psi$, so that for each fixed length sequence of vectors, the function predicts the correct set of possible subsequent classes. If one of the top $q$ candidate classes matches the actual class, then $\hat{b_i} = 0$ is returned, if none match the actual class, $\hat{b_i} = 1$ is returned.

\textit{Regression}: Let $h(E_i, \Phi): \left|s\right| \times \mathbb{R}^w \rightarrow \mathbb{R}^w$ be a function computed by a neural network, that based on a sequence of $s$ vectors $E_i = (e_i, ..., e_{i+s})$ predicts the vector $d$ at index $i+s+1$. The objective in this case is to learn parameters $\Phi$, so that the system predicts the vector following the sequence of vectors of length $s$. If the distance between the predicted vector $d$ and the actual vector $e_{i+s+1}$ is above or below threshold values, which will be computed by the q-th percentile of all loss values from the original dataset, then $\hat{b}_{i+s+1} = 1$ is returned, if it is inside these thresholds, then $\hat{b}_{i+s+1} = 0$ is returned.

\subsection{Requirements and Assumptions \label{sec:requirements_and_assumptions}}
\begin{enumerate}
	\item The model requires word vectors that are computed by a language model that has been pre-trained on a sufficiently large corpus.
	\item The model requires \textit{normal}, non-anomalous log data, which do not contain anomalies for training.
	\item The model is not able to detect anomalies which are only detectable in the variable part of the log messages. The model only considers the templates, i.e. keys of the log message.
\end{enumerate}



\section{System Overview \label{sec:overall_system}}
In this section, a broad overview of the overall system is presented, with figure \ref{fig:overall_system} illustrating the steps necessary for the learning procedure, followed by anomaly prediction. 
The core concept can be outlined as follows: first, the original log sequences are ordered, then a log parser is used to extract templates $t_i$ from the original log sequences $(m_0, m_1, ..., m_n)$ and then transform the log sequences to template sequences $(t_0, t_1, ..., t_n)$. Afterwards, the template sequences are transformed into log sequence embeddings $(e_0, e_1, ..., e_n)$ by a language representation model $M$. This procedure is described in detail in section \ref{sec:pre_processing}.

For the training part, the sub-sequences of log embeddings $E_i=(e_i,...,e_{i+s})$ are fed into a Bi-LSTM, which learns to predict the next embedding $e_{i+s+1}$, which is called the regression task, specified in section \ref{sec:regression} and coloured red in figure \ref{fig:overall_system} or the next template class $c_{i+s+1}$, which is called the classification task, specified in section \ref{sec:classification} and coloured blue in figure \ref{fig:overall_system}. The results of these predictions are then compared to the true subsequent log embedding or class of the true subsequent log template, in order to train the model to identify non-anomalous log data.

The prediction part involves two steps: first, the template sequences obtained from the original log sequences A are altered by changing the order of the sequences or manipulating the templates, as described in section \ref{sec:logs_alteration}. As a second step, these manipulated sequences are fed to the model which has been trained on embedding sequences not containing any alterations, thereby obtaining the model's predictions $\hat{b}$.

Transfer of knowledge builds onto this process. After a model has been trained as described on dataset $A$, pre-processing is conducted the same way on a new dataset $B$, thus obtaining a sequences of embeddings. For the classification task, the template sequences of dataset $B$ are mapped to the class mappings of dataset $A$, and then used for prediction. The regression task functions analogously to learning without the transfer mechanism. Transfer of knowledge is described in detail in section \ref{sec:transferlearning}

\begin{figure}[h]
	\centering
	\includegraphics[width=1\columnwidth]{overview_new.pdf}
	\caption{Anomaly Detection System}
	\label{fig:overall_system}
\end{figure}

\newpage
\section{Pre processing \label{sec:pre_processing}}
Log files are usually available in an unordered, raw state, and need to be ordered, parsed and transformed into an appropriate format so that they can be handled as sequences by a Bi-LSTM. The required steps for this purpose are outlined in this section. In the first subsection \ref{sec:conceptlogparsing}, the steps required for parsing logs in order to obtain log templates $t_i$ are outlined, followed by the transformation into word embedding vectors $e_i$ in \ref{sec:word_vectorization}. A diagram illustrating the complete pre-processing pipeline can be see in figure \ref{fig:full_preprocessing_pipeline}.

\subsection{Log Parsing}\label{sec:conceptlogparsing}
Log parsing is an important step for automated anomaly detection, as already described in section \ref{sec:backgroundlogparsing}, since raw log messages are unstructured data and contain a lot of extra information. 
For this work, the log parser Drain \cite{he2017drain} is utilised. It is characterised by high accuracy and efficiency \cite{he2017drain} and achieves the best results compared with other related methods, evaluated in \cite{zhu2019tools}. The detailed result of the execution of a log parser can be see in figure \ref{fig:parsing}. 
There are a few important aspects to note: Not only does the log parsing step extract log templates, but it also extracts other valuable information in a structured way, namely timestamps and, in this example the bulk id. Timestamps are needed, in order to make sure that the logs are in the correct chronological order, since it is possible, that system logs are an aggregation of log output of different sub-routines or different instances, which can happen concurrently, thus producing unordered logs. Additionally to sorting by timestamps, it can also be required to sort system logs by group ids, instance ids or bulk ids as it can be seen in step 1) in figure \ref{fig:full_preprocessing_pipeline}. Matching instance ids are identified, which is made visible by marking same instance ids with yellow. They are then separated in order to be able to observe each self-contained block separately from other blocks. After these sorting procedures, the result of the log parser's transformation $\mathcal{T}: m_i \to \mathbf{t_i}, \forall i$ from log messages to templates are visible as step 2) in figure \ref{fig:full_preprocessing_pipeline}.

\subsection{Template cleansing \label{sec:template_cleansing}}
Even though log parsing and the aforementioned ordering steps largely improve the further processability of logs for sequential learning, by making it possible to single out the fundamental semantics of a log event, they are still partly made up of special characters and variable names. The following characters are removed:
\begin{enumerate}
	\item All non-character tokens such as delimiters, digits, and particularly variable placeholders (\verb!<*>!).
	\item All concatenations of words are split, for example \verb!sync_power_state! will be split into the separate words \verb!sync!, \verb!power! and \verb!state!
	\item All leading, trailing and repeated whitespace characters are removed.
\end{enumerate}

This part of the pre-processing is marked with step 3) in figure \ref{fig:full_preprocessing_pipeline}.

\subsection{Word vectorisation \label{sec:word_vectorization}}
After the aforementioned pre-processing steps, the log events are transformed into word embeddings, using a language model that is able to convert words or whole sentences into word or sentence embedding, effectively representing function $M(t_i)$ defined in \ref{sec:formal_problem_definition}. The process of transforming a sequences of logs with the API of a language model is visualised in figure \ref{fig:transform_sequence_to_embeddings}.

Satisfying the following two requirements is essential in the context of providing suitable word embeddings for the anomaly detection task:
\begin{itemize}
	\item Distinguishability: Word embeddings should capture the difference between log events with differing semantics. For example "\verb!<*> Terminating instance!" and "\verb!<*> Deleting instance files <*>!" are log events with highly different semantics, even though they contain equal (instance) and in the broader sense similar (terminating, deleting) words. This means their cosine distance should be high.
	\item Tolerance: Word embeddings should capture the similarity between different log events with same or very similar semantics. For example, the log event pair "\verb!<*>! \verb!Creating image!" and "\verb!<*> Image created successfully!" or "\verb!VM up!" and \\ "\verb!VM started!". This in turn should result in a low cosine distance \cite{zhang2019robust}.
\end{itemize}

In oder to be able to compare the quality of different language model's word embeddings with regards to the task of anomaly detection, it is possible to easily swap the used language model for log event transformation. For this work, the pre-trained language models used for evaluation are Bert, GPT-2 and XL-Transformers. 


\begin{figure}[H]
	\centering
	\includegraphics[width=1\columnwidth]{transform_templates_to_embeddings.pdf}
	\caption{Transformation of a log event sequence to a word embeddings sequence.}
	\label{fig:transform_sequence_to_embeddings}
\end{figure}



\subsection{Finetuning\label{sec:finetuning}}
Word embeddings models are usually provided pre-trained in different formats (e.g. with or without upper case), because training them from scratch is expensive - for Bert, it requires 4 days of training on 16 cloud TPUs for one language \cite{googlebert}. They are trained on large corpuses with unsupervised tasks. In order to make them more useful with regards to the task of anomaly detection, it is reasonable, to adjust the given pre-trained datasets using the log data corpora, by using the finetuning API provided by the language models. For this purpose, the log corpus on which finetuning shall be executed is prepared for the finetuning task of Masked Language Model, as described in subsection \ref{Bert}.

\subsection{Log Alteration\label{sec:logs_alteration}}
By altering log events, the evolution and instability of log events is being simulated. Since software is changed by developers, also the log statements are subject to constant change. It is therefore necessary to build a flexible model that does not have to be retrained completely after each update of a log producing software. Log alteration is also used to simulate a different dataset $B$ based on a given dataset $A$, for the evaluation of the model on drastically changed logs that appear like from another dataset, as outlined in \ref{sec:transferlearning}.

Anomalies and alterations can be injected at arbitrary ratios. Various types of alterations are injected into original log data, either on the log event itself as depicted in figure \ref{fig:changelogevent} or on the sequence of log events as visualised in figure \ref{fig:changesequence}.

Three types of alterations are injected into the log events: a various amount of words is inserted between the tokens, for example words that appear in the context of logs, like "\textit{deleted}", "\textit{during}", "\textit{for}" or "\textit{time}". Words can also be also deleted or replaced. It is not expected that the system detects a log event as an anomaly, that has not been changed much, i.e. only one word has been added into a statement (e.g. if "\textit{* Took *.* seconds to deallocate network for instance.}" has been changed to "\textit{* Took *.* seconds to deallocate network for \textcolor{red}{this} instance.}").

Additionally, it is possible to perform changes on the sequence of logs. In the following example, let $M = (m_i : i = 0, 1, 2, ..., n))$ be a sequence of log events:
\begin{itemize}
	\setlength\itemsep{-0.5em}
	\item events can be \textit{deleted} from the sequence, meaning that if the event at index $j$ is selected for deletion, the resulting sequence is $M_{del} = (m_0, ..., m_{j-1}, m_{j+1}, m_n)$.
	\item events can be \textit{shuffled}, meaning that if the event index $j$ is selected for shuffling at index $k$, the resulting sequence is $M_s = (m_0, ..., m_j, m_k, m_{k+1}, ..., m_{j-1}, m_{j+1}, ..., m_n)$
	\item events can be \textit{duplicated}, meaning that if the event at index $j$ is selected for duplication, the resulting sequence is $M_{dup} = (m_0, ..., m_j, m_j, m_{j+1}, ..., m_n)$
	%\item new events can be \textit{inserted} meaning that if the event $m_{new}$ is inserted at index $j$, the resulting sequence is $M_{ins} = (m_0, ..., m_{new}, m_j, m_{j+1}, ..., m_n)$
\end{itemize}

Similar to the insertion of minimal alterations on the log events themselves, it is expected of the system not to detect an anomaly for the deletion, duplication or shuffling of events, since these also can reflect minor changes through new valid execution paths due to version updates and new deployments.
\vfill

\begin{figure}[H]
	\centering
	\includegraphics[width=11cm, height=7.5cm]{change_log_event.pdf}
	\caption{Altering log events.}
	\label{fig:changelogevent}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=14cm,height=6.8cm]{change_sequences.pdf}
	\caption{Altering log sequences.}
	\label{fig:changesequence}
\end{figure}

\begin{comment}
\begin{figure}[!tbp]
  \centering
  \subfloat[Altering log events.]{\includegraphics[width=0.3\textwidth]{change_log_event.pdf}\label{fig:changelogevent}}
  \hfill
  \subfloat[Altering log sequences.]{\includegraphics[width=0.65\textwidth]{change_sequences.pdf}\label{fig:changesequence}}
  \caption{Alteration of log events and sequences.}
\end{figure}
\end{comment}


\section{Prediction Model\label{sec:prediction_model}}

\subsection{LSTM Model\label{sec:lstm-model}}
Through the aforementioned steps in \ref{sec:pre_processing}, the system log $L$ has been transformed into a sequence of word embeddings $e_i \in \mathbb{R}^w$. In order to learn sequences of logs, a sequence of $s$ log embeddings are concatenated to form an embedding sequence $E_i = (e_i,...,e_{i+s})$. Taking a sequence of embeddings as input, a \textit{Bi-LSTM} neural network as described in \ref{sec:bi-lstm} is utilised to predict the class or embedding at position $i+s+1$. Figure \ref{fig:lstm_model} shows the structure of the Bi-LSTM. As a first step, a dropout is applied to the input sequence, which randomly drops out information in order to reduce overfitting and improve generalisation, before feeding it to the forward and backward layers of the Bi-LSTM. This way, more information of the input log sequences can be captured, than when only an uni-directional LSTM would be utilised. Then, the outputs of the Bi-LSTM are again fed into a dropout layer, followed by a fully connected layer, which reduces the output of the LSTM into the desired dimensions, i.e. $\mathbb{R}^w$ for regression and number of classes $c$ for classification. Finally, an activation function $act$ is applied to the last output $o_{i+s+1}$, \textit{log-softmax} for classification and \textit{linear} for regression, respectively, in order to obtain the model's prediction $p_{i+s+1}$.


\begin{figure}[H]
	\centering	
	\includegraphics[width=16cm]{lstm_model.pdf}
	\caption{Bi-LSTM model}
	\label{fig:lstm_model}
\end{figure}
\newpage
\subsection{Classification \label{sec:classification}}
For the classification approach, the finite set of $n$ unique log event templates is mapped to class indices $c_0, ..., c_n$. Training of the neural network is then performed on original log data that does not contain anomalies.
\begin{itemize}
\setlength\itemsep{-0.5em}
	\item The \textit{input} values of the training data have the dimensions $n$, $s$ and $w$, with $n$ being the number of unique log events, $s$ being the length of the sequence of word embeddings for which the neural network shall learn the consequent class and $w$ being the dimensionality of the log event embeddings.
	\item The \textit{target} values of the training data are structured as follows: for every sequence of word embeddings $E_i = (i, ..., i+s)$, there is a corresponding class $c_{s+i+1}$ that stands for the template at position $s+i+1$. The system is trained to predict that class correctly.
\end{itemize}

After training has been executed on the \textit{train} dataset, the prediction phase starts, where a \textit{test} dataset containing anomalies and alterations, as described in \ref{sec:logs_alteration}, can be processed by the neural network. For a new incoming test dataset, the system first has to match every template to the template classes of the train dataset. For every template in the test dataset, the nearest neighbour out of the templates of the train dataset is determined and will get the respective class assigned, as depicted in figure \ref{fig:label_mapping}. This means in particular, that for every unique template, the corresponding word embedding is retrieved, and then every one of the word embeddings of the test dataset is mapped to the word embedding from the train dataset with the lowest cosine distance. Additionally, a threshold has to be found, so that if for a given embedding in the test data, no corresponding embedding with a cosine distance below this threshold is found, that template shall not get a class assigned, this would otherwise lead to a situation where the log event gets mapped to any of the template classes, which is not desirable behaviour.

After the matching process is finished, the system can read in the sequences of log events of the test data. For every sequence of log events $E_i$ of length $s$, the system returns a probability distribution $Pr[c_{s+i+1}|E_i] = \{c_0 : p_o, c_1 : p_1, ..., c_n : p_n\}$ that denotes the probability of each log template class to appear as the subsequent one. It is possible, that there are multiple candidates for the following template. Let's assume, that the system is attempting to terminate an instance, then the corresponding template to class $c_{s+i+1}$ could be for example '\textit{Instance terminated successfully}' or '\textit{Waiting for instance to terminate}'. The possible template classes $c_i$ are sorted based on their probabilities. A predicted template class is considered normal, resulting in the system's prediction $\hat{b}_{s+i+1}=0$, if it is among the top $q$ candidates. It is marked as anomalous, $\hat{b}_{s+i+1}=1$ otherwise.
 

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=15cm]{label_mapping.pdf}
	\caption{Template mapping}
	\label{fig:label_mapping}
\end{figure}

\begin{figure}[H]
	\centering	
	\includegraphics[width=15cm]{classification_detect.pdf}
	\caption{Class Prediction example for $g=3$}
	\label{fig:class_prediction}
\end{figure}
\newpage
\subsection{Regression \label{sec:regression}}
For the regression approach, the neural network is trained to solve a regression task, with the input values of the training data being structured as described in \ref{sec:classification}, while the corresponding target value for the sequence of embeddings $E_i = (e_i, ..., e_{i+s})$ is the embedding of the log event at position $i+s+1$, meaning the neural network shall predict the next embedding $e_{i+s+1}$. After training on the original dataset, the mean squared error loss of every target word vector at position $i+s+1$ of the corresponding input sequence $E_i$, and the neural network's predicted word vector, is computed. Afterwards, the \textit{q-th} percentile of the agglomerated loss values of the original dataset is computed. The optimal value \textit{q} for the following purpose is to be determined by performing a grid-search. For the sequences of the test dataset, the loss values are computed in the same way as for the original dataset. This is depicted in figure \ref{fig:regression_with_threshold}. The system will then mark every log event at position $i$ whose word embedding's loss value is above the calculated \textit{q-th} percentile as an anomaly, with $\hat{b}_i=1$, otherwise as non anomalous, with $\hat{b}_i=0$.

\begin{figure}[h]
  \centering
  \includegraphics[width=15cm]{regression_box_plots_example.pdf} \\
  \caption{Box plots showing the distribution of loss values for training and test data.}
  \label{fig:regression_with_threshold}
\end{figure}

\newpage
\section{Transfer of Knowledge \label{sec:transferlearning}}
As already mentioned in chapter \ref{cha:introduction}, modern cloud systems and their underlying software are subject to constant change. Through continuous re-deployment and updates of services and systems, there is a lack of sufficient data to perform training of a new anomaly detection model. Therefore, using pre-trained general purpose language models for extracting vector representations of logs and re-usage of already pre-initialised weights of a Bi-LSTM model from training on older log version allows transferring the model to new unseen logs. Let dataset $A$ be the training dataset from already known log messages, and dataset $B$ be a log dataset from an updated version of the system. The steps for prediction of unseen logs using the classification approach are outlined in subsection \ref{sec:transfer_classification} and for the regression approach in subsection \ref{sec:transfer_regression}.

\begin{comment}
\begin{figure}[H]
	\centering
	\includegraphics[width=15cm]{transfer-learning.png}
	\caption{Transfer of Knowledge System}
	\label{fig:transfer_learning_system}
\end{figure}
\end{comment}

\subsection{Classification \label{sec:transfer_classification}}
For the classification approach, the model is first trained as depicted in the pre-processing and training part of figure \ref{fig:overall_system} on a train dataset $A$. Then, in order to re-use the trained model, several preliminary steps are executed. First, every log event of a training dataset $B$ is mapped to the nearest neighbour of train dataset $A$, i.e. the word embedding with the shortest cosine distance, and gets assigned the same class. This is the same procedure as described in \ref{sec:classification} and depicted in \ref{fig:label_mapping}, with the only difference that there does not exist a threshold for assigning a class. Every log event will get a class assigned. Then, few-shot training on the training dataset $B$ will be executed, in order to adjust the model to the new dataset $B$. As a final step, with the adjusted model on training dataset $B$, the prediction phase on a test dataset $B$ can be executed, completely analogous to the description in \ref{sec:classification} and as depicted in \ref{fig:overall_system}.

\subsection{Regression \label{sec:transfer_regression}}
For transfer of knowledge using the regression-based approach, the model is first trained as depicted in the pre-processing and training part of figure \ref{fig:overall_system} on a train dataset $A$. Then, the same weights of the LSTM that have been learned from this training are re-loaded, and few-shot training on a different train dataset $B$ is executed. The model is then ready to make predictions on anomalies on a test dataset $B$ using the approach described in \ref{sec:regression}.



\begin{figure}[htb]
  \centering
  \includegraphics[width=15cm]{parsing_full.pdf}\\
  \caption{One full iteration of the pre-processing pipeline.}
  \label{fig:full_preprocessing_pipeline}
\end{figure}







